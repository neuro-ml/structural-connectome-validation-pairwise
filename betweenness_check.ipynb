{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'igraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3f13abcd6d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0migraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'igraph'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import *\n",
    "from scipy.sparse.csgraph import dijkstra, shortest_path, connected_components, laplacian\n",
    "\n",
    "from sklearn.base import  BaseEstimator, TransformerMixin\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import scipy\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Sep 10 2016, 08:21:44) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d09c7b568de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mpairs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/parkinson_pairs_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_even_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################\n",
    "# Normalizer's functions #\n",
    "##########################\n",
    "\n",
    "def no_norm(matrix):\n",
    "    return matrix\n",
    "\n",
    "def max_norm(matrix):\n",
    "    normed_matrix = matrix / np.max(matrix)\n",
    "    return normed_matrix\n",
    "\n",
    "def binar_norm(matrix):\n",
    "    bin_matrix = matrix.copy()\n",
    "    bin_matrix[bin_matrix > 0] = 1\n",
    "    return bin_matrix\n",
    "\n",
    "def mean_norm(matrix):\n",
    "    normed_matrix = matrix / np.mean(matrix)\n",
    "    return normed_matrix\n",
    "\n",
    "def double_norm(function, matrix1, matrix2):\n",
    "    return function(matrix1), function(matrix2)\n",
    "\n",
    "##########################\n",
    "# Featurizer's functions #\n",
    "##########################\n",
    "\n",
    "def bag_of_edges(X, SPL=None, symmetric = True, return_df = False, offset = 1):\n",
    "    size = X.shape[1]\n",
    "    if symmetric:\n",
    "        indices = np.triu_indices(size, k = offset)\n",
    "    else:\n",
    "        grid = np.indices(X.shape[1:])\n",
    "        indices = (grid[0].reshape(-1), grid[1].reshape(-1))\n",
    "    if len(X.shape) == 3:\n",
    "        featurized_X = X[:, indices[0], indices[1]]\n",
    "    elif len(X.shape) == 2:\n",
    "        featurized_X = X[indices[0], indices[1]]\n",
    "    else:\n",
    "        raise ValueError('Provide array of valid shape: (number_of_matrices, size, size).')\n",
    "    if return_df:\n",
    "        col_names = ['edge_' + str(i) + '_' + str(j) for i,j in zip(indices[0], indices[1])]\n",
    "        featurized_X = pd.DataFrame(featurized_X, columns=col_names)\n",
    "    return featurized_X\n",
    "\n",
    "def degrees(X, return_df = False):\n",
    "    if len(X.shape) == 3:\n",
    "        featurized_X = np.sum(X, axis=1)\n",
    "        shape = (X.shape[0], X.shape[1])\n",
    "    elif len(X.shape) == 2:\n",
    "        featurized_X = np.sum(X, axis=1)\n",
    "        shape = (1, X.shape[1])\n",
    "    else:\n",
    "        raise ValueError('Provide array of valid shape: (number_of_matrices, size, size). ')\n",
    "\n",
    "    if return_df:\n",
    "        col_names = ['degree_' + str(i) for i in range(X.shape[1])]\n",
    "        featurized_X = pd.DataFrame(featurized_X.reshape(shape), columns=col_names)\n",
    "    return featurized_X\n",
    "\n",
    "def closeness_centrality(X):\n",
    "    n_nodes = X.shape[0]\n",
    "    A_inv = 1./X\n",
    "    SPL = scipy.sparse.csgraph.dijkstra(A_inv, directed=False,\n",
    "            unweighted=False)\n",
    "    sum_distances_vector = np.sum(SPL, 1)\n",
    "    cl_c = float(n_nodes - 1)/sum_distances_vector\n",
    "    featurized_X = cl_c\n",
    "    return featurized_X\n",
    "\n",
    "def betweenness_centrality(X):\n",
    "    n_nodes = X.shape[0]\n",
    "    A_inv = 1./X\n",
    "    G_inv = ig.Graph.Weighted_Adjacency(list(A_inv), mode=\"UNDIRECTED\", attr=\"weight\", loops=False)\n",
    "    btw = np.array(G_inv.betweenness(weights='weight', directed=False))*2./((n_nodes-1)*(n_nodes-2))\n",
    "    return btw\n",
    "\n",
    "def eigenvector_centrality(X):\n",
    "    G = ig.Graph.Weighted_Adjacency(list(X), mode=\"UNDIRECTED\",\n",
    "                attr=\"weight\", loops=False)\n",
    "    eigc = G.eigenvector_centrality(weights='weight', directed=False)\n",
    "    return np.array(eigc)\n",
    "\n",
    "def pagerank(X):\n",
    "    G = ig.Graph.Weighted_Adjacency(list(X), mode=\"DIRECTED\", attr=\"weight\", loops=False)\n",
    "    return np.array(G.pagerank(weights=\"weight\"))\n",
    "\n",
    "def efficiency(X):\n",
    "    A_inv = 1./X\n",
    "    SPL = scipy.sparse.csgraph.dijkstra(A_inv, directed=False, unweighted=False)\n",
    "    inv_SPL_with_inf = 1./SPL\n",
    "    inv_SPL_with_nan = inv_SPL_with_inf.copy()\n",
    "    inv_SPL_with_nan[np.isinf(inv_SPL_with_inf)]=np.nan\n",
    "    efs = np.nanmean(inv_SPL_with_nan, 1)\n",
    "    return efs\n",
    "\n",
    "def clustering_coefficient(X):\n",
    "    Gnx = nx.from_numpy_matrix(X)\n",
    "    clst_geommean = list(nx.clustering(Gnx, weight='weight').values())\n",
    "    clst_geommean\n",
    "    return np.array(clst_geommean)\n",
    "\n",
    "def triangles(X):\n",
    "    clust = clustering_coefficient(X)\n",
    "\n",
    "    G = ig.Graph.Weighted_Adjacency(list(X), mode=\"UNDIRECTED\",\n",
    "            attr=\"weight\", loops=False)\n",
    "    non_weighted_degrees = np.array(G.degree())\n",
    "    non_weighted_deg_by_deg_minus_one = np.multiply(non_weighted_degrees,\n",
    "            (non_weighted_degrees - 1))\n",
    "    tr = np.multiply(np.array(clust),\n",
    "            np.array(non_weighted_deg_by_deg_minus_one, dtype = float))/2.\n",
    "    return tr\n",
    "\n",
    "\n",
    "########################\n",
    "# TRANSFORMERS CLASSES #\n",
    "########################\n",
    "\n",
    "pairs_data = pd.read_csv('../data/parkinson_pairs_data.csv', index_col = None)\n",
    "\n",
    "def generate_even_sample(data, n = 1000, seed = 0):\n",
    "    sample_of_1 = data[data.are_same == 1].sample(n=n, random_state=seed)\n",
    "    sample_of_0 = data[data.are_same == 0].sample(n=n, random_state=seed)\n",
    "    return pd.concat([sample_of_1, sample_of_0], axis=0)\n",
    "\n",
    "class DataTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target, copy=True):\n",
    "        self.target = target\n",
    "\n",
    "    def fit(self, pairs_data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, pairs_data, data_dir='../data/parkinson/'):\n",
    "        matrices = {}\n",
    "        if self.target != 'All':\n",
    "            pairs_data = pairs_data[(pairs_data.subject1_target == self.target) & (pairs_data.subject2_target == self.target)]\n",
    "        file_ids = np.unique(pairs_data[['subject1_id', 'subject2_id']])\n",
    "        for file_id in file_ids:\n",
    "            for file in os.listdir(data_dir+file_id):\n",
    "                if 'FULL' in file:\n",
    "                    print(data_dir + file_id + '/' + file)\n",
    "                    matrix = np.loadtxt(data_dir + file_id + '/' + file)\n",
    "                    matrix = np.delete(matrix, [3,38], axis = 1)\n",
    "                    matrix = np.delete(matrix, [3,38], axis = 0)\n",
    "                    np.fill_diagonal(matrix, 0)\n",
    "                    matrices[file_id] = matrix\n",
    "\n",
    "        pairs_data = generate_even_sample(pairs_data, n = int(pairs_data.are_same.sum()))\n",
    "        return {'pairs_data': pairs_data,\n",
    "                'matrices': matrices}\n",
    "\n",
    "class MatrixNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, norm, copy=True):\n",
    "        self.norm    = norm\n",
    "        self.copy    = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = {}\n",
    "\n",
    "        for key in X['matrices'].keys():\n",
    "            X_transformed[key] = self.norm(X['matrices'][key])\n",
    "\n",
    "        return {'pairs_data': X['pairs_data'],\n",
    "                'matrices': X_transformed}\n",
    "\n",
    "class MatrixFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features, copy=True):\n",
    "        self.features = features\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        cur_features = {}\n",
    "        for key in X['matrices'].keys():\n",
    "            cur_features[key] = self.features[0](X['matrices'][key])\n",
    "            for feature_func in self.features[1:]:\n",
    "                cur_features[key] = np.append(cur_features[key], feature_func(X['matrices'][key]))\n",
    "\n",
    "        return {'pairs_data': X['pairs_data'],\n",
    "                'features': cur_features}\n",
    "\n",
    "def gen_dist(p): return lambda x,y: minkowski(x.reshape(-1),y.reshape(-1),p)\n",
    "func_list = [chebyshev] + [gen_dist(i) for i in [1, 2]]\n",
    "\n",
    "class VectorFeaturizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, func_list):\n",
    "        self.func_list = func_list\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectors1 = X['pairs_data'].subject1_id.apply(lambda x: X['features'][x])\n",
    "        vectors2 = X['pairs_data'].subject2_id.apply(lambda x: X['features'][x])\n",
    "        features = []\n",
    "        for index in vectors1.index:\n",
    "            feats = []\n",
    "            for function in self.func_list:\n",
    "                feats.append(function(vectors1[index], vectors2[index]))\n",
    "            features.append(feats)\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "######################\n",
    "# PARAMETERS SETTING #\n",
    "######################\n",
    "\n",
    "sys.path.append(os.path.abspath('../../../Reskit/'))\n",
    "\n",
    "from reskit.core import *\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_cv = StratifiedKFold(n_splits=10, shuffle=True,  random_state=0)\n",
    "\n",
    "eval_cv = StratifiedShuffleSplit(\n",
    "            n_splits = 100,\n",
    "            test_size = 0.2,\n",
    "            random_state = 0 )\n",
    "\n",
    "steps = [\n",
    "    ('norm', [\n",
    "        ('-', [\n",
    "            ('object', MatrixNormalizer),\n",
    "            ('params', [\n",
    "                ('param', {'norm': no_norm}) ]) ]),\n",
    "        ('max', [\n",
    "            ('object', MatrixNormalizer),\n",
    "            ('params', [\n",
    "                ('param', {'norm': max_norm}) ]) ]),\n",
    "        ('binar', [\n",
    "            ('object', MatrixNormalizer),\n",
    "            ('params', [\n",
    "                ('param', {'norm': binar_norm}) ]) ]),\n",
    "        ('mean', [\n",
    "            ('object', MatrixNormalizer),\n",
    "            ('params', [\n",
    "                ('param', {'norm': mean_norm}) ]) ]) ]),\n",
    "    ('base_features', [\n",
    "        ('bag_of_edges', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [bag_of_edges]}) ]) ]),\n",
    "        ('degrees', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [degrees]}) ]) ]),\n",
    "        ('closeness_centrality', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [closeness_centrality]}) ]) ]),\n",
    "        ('betweenness_centrality', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [betweenness_centrality]}) ]) ]),\n",
    "        ('eigenvector_centrality', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [eigenvector_centrality]}) ]) ]),\n",
    "        ('pagerank', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [pagerank]}) ]) ]),\n",
    "        ('efficiency', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [efficiency]}) ]) ]),\n",
    "        ('clustering_coefficient', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [clustering_coefficient]}) ]) ]),\n",
    "        ('triangles', [\n",
    "            ('object', MatrixFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'features': [triangles]}) ]) ]) ]),\n",
    "    ('pairwise_features', [\n",
    "        ('l1_l2_linf', [\n",
    "            ('object', VectorFeaturizer),\n",
    "            ('params', [\n",
    "                ('param', {'func_list': func_list}) ]) ]) ]),\n",
    "    ('scaler', [\n",
    "        ('standard', [\n",
    "            ('object', StandardScaler),\n",
    "            ('params', [\n",
    "                ('None', {}) ]) ]) ]),\n",
    "    ('classifier', [\n",
    "        ('grid', [\n",
    "            ('object', GridSearchCV),\n",
    "            ('params', [\n",
    "                ('LR', {\n",
    "                    'estimator': LogisticRegression(),\n",
    "                    'cv': grid_cv,\n",
    "                    'n_jobs': -1,\n",
    "                    'param_grid': {\n",
    "                        'penalty': ['l1', 'l2'],\n",
    "                        'C':[0.05*i for i in range(1,20)],\n",
    "                        'fit_intercept': [True],\n",
    "                        'max_iter': [50, 100],\n",
    "                        'random_state': [0],\n",
    "                        'solver': ['liblinear'],\n",
    "                        'n_jobs': [1] } }),\n",
    "                ('RF', {\n",
    "                    'estimator': RandomForestClassifier(),\n",
    "                    'cv': grid_cv,\n",
    "                    'n_jobs': -1,\n",
    "                    'param_grid': {\n",
    "                        'n_estimators': [100],\n",
    "                        'criterion': ['gini', 'entropy'],\n",
    "                        'max_depth': [2, 3, 3, 5, 7],\n",
    "                        'min_samples_split': [2],\n",
    "                        'min_samples_leaf': [1],\n",
    "                        'min_weight_fraction_leaf': [0.0],\n",
    "                        'max_features': [0.5, 1],\n",
    "                        'max_leaf_nodes': [None],\n",
    "                        'bootstrap': [True],\n",
    "                        'oob_score': [False],\n",
    "                        'n_jobs':[1],\n",
    "                        'random_state': [0],\n",
    "                        'verbose': [0],\n",
    "                        'warm_start': [False],\n",
    "                        'class_weight': [None] } }),\n",
    "                ('GBT', {\n",
    "                    'estimator': XGBClassifier(),\n",
    "                    'cv': grid_cv,\n",
    "                    'n_jobs': -1,\n",
    "                    'param_grid': {\n",
    "                        'max_depth': [2,3,4,5],\n",
    "                        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.5],\n",
    "                        'n_estimators': [100],\n",
    "                        'silent': [True],\n",
    "                        'objective': ['binary:logistic'],\n",
    "                        'nthread': [1],\n",
    "                        'gamma': [0],\n",
    "                        'subsample': [1.0, 0.9, 0.8, 0.7, 0.6],\n",
    "                        'colsample_bytree': [1.0],\n",
    "                        'base_score': [0.5],\n",
    "                        'seed': [0] } }),\n",
    "                ('SGD', {\n",
    "                    'estimator': SGDClassifier(),\n",
    "                    'cv': grid_cv,\n",
    "                    'n_jobs': -1,\n",
    "                    'param_grid': {\n",
    "                        'loss':['hinge', 'log', 'modified_huber'],\n",
    "                        'penalty': ['elasticnet'],\n",
    "                        'alpha': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "                        'l1_ratio': [0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "                        'fit_intercept': [True],\n",
    "                        'n_iter': [50, 100, 200],\n",
    "                        'shuffle': [True],\n",
    "                        'verbose':[0],\n",
    "                        'epsilon': [0.1],\n",
    "                        'n_jobs': [-1],\n",
    "                        'random_state':[0],\n",
    "                        'learning_rate': ['optimal'],\n",
    "                        'eta0': [0.0],\n",
    "                        'power_t': [0.5],\n",
    "                        'class_weight': [None] } }),\n",
    "                ('SVC', {\n",
    "                    'estimator': SVC(),\n",
    "                    'cv': grid_cv,\n",
    "                    'n_jobs': -1,\n",
    "                    'param_grid': {\n",
    "                        'C': [0.05*i for i in range(1,11)],\n",
    "                        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                        'degree': list(range(2,5)),\n",
    "                        'gamma': [0.01, 0.001, 0.05, 0.1, 0.2],\n",
    "                        'max_iter': [50, 100],\n",
    "                        'random_state': [0],\n",
    "                        'shrinking': [True] } }) ]) ]) ]) ]\n",
    "\n",
    "scoring = ['accuracy', 'roc_auc']\n",
    "targets = ['All', 'PD', 'Control']\n",
    "steps = Steps(steps)\n",
    "\n",
    "cfg = Config(   steps = steps,\n",
    "                eval_cv = eval_cv,\n",
    "                scoring = scoring   )\n",
    "\n",
    "output_folder = 'PPMI_pairwise_results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for target in targets:\n",
    "    pp = Pipeliner( config = cfg )\n",
    "    y = DataTransformer( target ).fit_transform( pairs_data )['pairs_data'].are_same\n",
    "    X = DataTransformer( target ).fit_transform( pairs_data )\n",
    "\n",
    "    pp.get_results( X, y.values,\n",
    "                    featuring_steps = [ 'norm',\n",
    "                                        'base_features',\n",
    "                                        'pairwise_features' ],\n",
    "                    results_file    = output_folder + target + '.csv' )\n",
    "    print('\\n\\n', target, 'Done')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
